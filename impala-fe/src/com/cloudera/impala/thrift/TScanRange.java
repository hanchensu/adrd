/**
 * Autogenerated by Thrift Compiler (0.9.0)
 *
 * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING
 *  @generated
 */
package com.cloudera.impala.thrift;

import org.apache.thrift.scheme.IScheme;
import org.apache.thrift.scheme.SchemeFactory;
import org.apache.thrift.scheme.StandardScheme;

import org.apache.thrift.scheme.TupleScheme;
import org.apache.thrift.protocol.TTupleProtocol;
import org.apache.thrift.protocol.TProtocolException;
import org.apache.thrift.EncodingUtils;
import org.apache.thrift.TException;
import java.util.List;
import java.util.ArrayList;
import java.util.Map;
import java.util.HashMap;
import java.util.EnumMap;
import java.util.Set;
import java.util.HashSet;
import java.util.EnumSet;
import java.util.Collections;
import java.util.BitSet;
import java.nio.ByteBuffer;
import java.util.Arrays;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class TScanRange implements org.apache.thrift.TBase<TScanRange, TScanRange._Fields>, java.io.Serializable, Cloneable {
  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct("TScanRange");

  private static final org.apache.thrift.protocol.TField HDFS_FILE_SPLIT_FIELD_DESC = new org.apache.thrift.protocol.TField("hdfs_file_split", org.apache.thrift.protocol.TType.STRUCT, (short)1);
  private static final org.apache.thrift.protocol.TField HBASE_KEY_RANGE_FIELD_DESC = new org.apache.thrift.protocol.TField("hbase_key_range", org.apache.thrift.protocol.TType.STRUCT, (short)2);

  private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();
  static {
    schemes.put(StandardScheme.class, new TScanRangeStandardSchemeFactory());
    schemes.put(TupleScheme.class, new TScanRangeTupleSchemeFactory());
  }

  public THdfsFileSplit hdfs_file_split; // optional
  public THBaseKeyRange hbase_key_range; // optional

  /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */
  public enum _Fields implements org.apache.thrift.TFieldIdEnum {
    HDFS_FILE_SPLIT((short)1, "hdfs_file_split"),
    HBASE_KEY_RANGE((short)2, "hbase_key_range");

    private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();

    static {
      for (_Fields field : EnumSet.allOf(_Fields.class)) {
        byName.put(field.getFieldName(), field);
      }
    }

    /**
     * Find the _Fields constant that matches fieldId, or null if its not found.
     */
    public static _Fields findByThriftId(int fieldId) {
      switch(fieldId) {
        case 1: // HDFS_FILE_SPLIT
          return HDFS_FILE_SPLIT;
        case 2: // HBASE_KEY_RANGE
          return HBASE_KEY_RANGE;
        default:
          return null;
      }
    }

    /**
     * Find the _Fields constant that matches fieldId, throwing an exception
     * if it is not found.
     */
    public static _Fields findByThriftIdOrThrow(int fieldId) {
      _Fields fields = findByThriftId(fieldId);
      if (fields == null) throw new IllegalArgumentException("Field " + fieldId + " doesn't exist!");
      return fields;
    }

    /**
     * Find the _Fields constant that matches name, or null if its not found.
     */
    public static _Fields findByName(String name) {
      return byName.get(name);
    }

    private final short _thriftId;
    private final String _fieldName;

    _Fields(short thriftId, String fieldName) {
      _thriftId = thriftId;
      _fieldName = fieldName;
    }

    public short getThriftFieldId() {
      return _thriftId;
    }

    public String getFieldName() {
      return _fieldName;
    }
  }

  // isset id assignments
  private _Fields optionals[] = {_Fields.HDFS_FILE_SPLIT,_Fields.HBASE_KEY_RANGE};
  public static final Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> metaDataMap;
  static {
    Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> tmpMap = new EnumMap<_Fields, org.apache.thrift.meta_data.FieldMetaData>(_Fields.class);
    tmpMap.put(_Fields.HDFS_FILE_SPLIT, new org.apache.thrift.meta_data.FieldMetaData("hdfs_file_split", org.apache.thrift.TFieldRequirementType.OPTIONAL, 
        new org.apache.thrift.meta_data.StructMetaData(org.apache.thrift.protocol.TType.STRUCT, THdfsFileSplit.class)));
    tmpMap.put(_Fields.HBASE_KEY_RANGE, new org.apache.thrift.meta_data.FieldMetaData("hbase_key_range", org.apache.thrift.TFieldRequirementType.OPTIONAL, 
        new org.apache.thrift.meta_data.StructMetaData(org.apache.thrift.protocol.TType.STRUCT, THBaseKeyRange.class)));
    metaDataMap = Collections.unmodifiableMap(tmpMap);
    org.apache.thrift.meta_data.FieldMetaData.addStructMetaDataMap(TScanRange.class, metaDataMap);
  }

  public TScanRange() {
  }

  /**
   * Performs a deep copy on <i>other</i>.
   */
  public TScanRange(TScanRange other) {
    if (other.isSetHdfs_file_split()) {
      this.hdfs_file_split = new THdfsFileSplit(other.hdfs_file_split);
    }
    if (other.isSetHbase_key_range()) {
      this.hbase_key_range = new THBaseKeyRange(other.hbase_key_range);
    }
  }

  public TScanRange deepCopy() {
    return new TScanRange(this);
  }

  @Override
  public void clear() {
    this.hdfs_file_split = null;
    this.hbase_key_range = null;
  }

  public THdfsFileSplit getHdfs_file_split() {
    return this.hdfs_file_split;
  }

  public TScanRange setHdfs_file_split(THdfsFileSplit hdfs_file_split) {
    this.hdfs_file_split = hdfs_file_split;
    return this;
  }

  public void unsetHdfs_file_split() {
    this.hdfs_file_split = null;
  }

  /** Returns true if field hdfs_file_split is set (has been assigned a value) and false otherwise */
  public boolean isSetHdfs_file_split() {
    return this.hdfs_file_split != null;
  }

  public void setHdfs_file_splitIsSet(boolean value) {
    if (!value) {
      this.hdfs_file_split = null;
    }
  }

  public THBaseKeyRange getHbase_key_range() {
    return this.hbase_key_range;
  }

  public TScanRange setHbase_key_range(THBaseKeyRange hbase_key_range) {
    this.hbase_key_range = hbase_key_range;
    return this;
  }

  public void unsetHbase_key_range() {
    this.hbase_key_range = null;
  }

  /** Returns true if field hbase_key_range is set (has been assigned a value) and false otherwise */
  public boolean isSetHbase_key_range() {
    return this.hbase_key_range != null;
  }

  public void setHbase_key_rangeIsSet(boolean value) {
    if (!value) {
      this.hbase_key_range = null;
    }
  }

  public void setFieldValue(_Fields field, Object value) {
    switch (field) {
    case HDFS_FILE_SPLIT:
      if (value == null) {
        unsetHdfs_file_split();
      } else {
        setHdfs_file_split((THdfsFileSplit)value);
      }
      break;

    case HBASE_KEY_RANGE:
      if (value == null) {
        unsetHbase_key_range();
      } else {
        setHbase_key_range((THBaseKeyRange)value);
      }
      break;

    }
  }

  public Object getFieldValue(_Fields field) {
    switch (field) {
    case HDFS_FILE_SPLIT:
      return getHdfs_file_split();

    case HBASE_KEY_RANGE:
      return getHbase_key_range();

    }
    throw new IllegalStateException();
  }

  /** Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise */
  public boolean isSet(_Fields field) {
    if (field == null) {
      throw new IllegalArgumentException();
    }

    switch (field) {
    case HDFS_FILE_SPLIT:
      return isSetHdfs_file_split();
    case HBASE_KEY_RANGE:
      return isSetHbase_key_range();
    }
    throw new IllegalStateException();
  }

  @Override
  public boolean equals(Object that) {
    if (that == null)
      return false;
    if (that instanceof TScanRange)
      return this.equals((TScanRange)that);
    return false;
  }

  public boolean equals(TScanRange that) {
    if (that == null)
      return false;

    boolean this_present_hdfs_file_split = true && this.isSetHdfs_file_split();
    boolean that_present_hdfs_file_split = true && that.isSetHdfs_file_split();
    if (this_present_hdfs_file_split || that_present_hdfs_file_split) {
      if (!(this_present_hdfs_file_split && that_present_hdfs_file_split))
        return false;
      if (!this.hdfs_file_split.equals(that.hdfs_file_split))
        return false;
    }

    boolean this_present_hbase_key_range = true && this.isSetHbase_key_range();
    boolean that_present_hbase_key_range = true && that.isSetHbase_key_range();
    if (this_present_hbase_key_range || that_present_hbase_key_range) {
      if (!(this_present_hbase_key_range && that_present_hbase_key_range))
        return false;
      if (!this.hbase_key_range.equals(that.hbase_key_range))
        return false;
    }

    return true;
  }

  @Override
  public int hashCode() {
    return 0;
  }

  public int compareTo(TScanRange other) {
    if (!getClass().equals(other.getClass())) {
      return getClass().getName().compareTo(other.getClass().getName());
    }

    int lastComparison = 0;
    TScanRange typedOther = (TScanRange)other;

    lastComparison = Boolean.valueOf(isSetHdfs_file_split()).compareTo(typedOther.isSetHdfs_file_split());
    if (lastComparison != 0) {
      return lastComparison;
    }
    if (isSetHdfs_file_split()) {
      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.hdfs_file_split, typedOther.hdfs_file_split);
      if (lastComparison != 0) {
        return lastComparison;
      }
    }
    lastComparison = Boolean.valueOf(isSetHbase_key_range()).compareTo(typedOther.isSetHbase_key_range());
    if (lastComparison != 0) {
      return lastComparison;
    }
    if (isSetHbase_key_range()) {
      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.hbase_key_range, typedOther.hbase_key_range);
      if (lastComparison != 0) {
        return lastComparison;
      }
    }
    return 0;
  }

  public _Fields fieldForId(int fieldId) {
    return _Fields.findByThriftId(fieldId);
  }

  public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException {
    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);
  }

  public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException {
    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);
  }

  @Override
  public String toString() {
    StringBuilder sb = new StringBuilder("TScanRange(");
    boolean first = true;

    if (isSetHdfs_file_split()) {
      sb.append("hdfs_file_split:");
      if (this.hdfs_file_split == null) {
        sb.append("null");
      } else {
        sb.append(this.hdfs_file_split);
      }
      first = false;
    }
    if (isSetHbase_key_range()) {
      if (!first) sb.append(", ");
      sb.append("hbase_key_range:");
      if (this.hbase_key_range == null) {
        sb.append("null");
      } else {
        sb.append(this.hbase_key_range);
      }
      first = false;
    }
    sb.append(")");
    return sb.toString();
  }

  public void validate() throws org.apache.thrift.TException {
    // check for required fields
    // check for sub-struct validity
    if (hdfs_file_split != null) {
      hdfs_file_split.validate();
    }
    if (hbase_key_range != null) {
      hbase_key_range.validate();
    }
  }

  private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException {
    try {
      write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));
    } catch (org.apache.thrift.TException te) {
      throw new java.io.IOException(te);
    }
  }

  private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException {
    try {
      read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));
    } catch (org.apache.thrift.TException te) {
      throw new java.io.IOException(te);
    }
  }

  private static class TScanRangeStandardSchemeFactory implements SchemeFactory {
    public TScanRangeStandardScheme getScheme() {
      return new TScanRangeStandardScheme();
    }
  }

  private static class TScanRangeStandardScheme extends StandardScheme<TScanRange> {

    public void read(org.apache.thrift.protocol.TProtocol iprot, TScanRange struct) throws org.apache.thrift.TException {
      org.apache.thrift.protocol.TField schemeField;
      iprot.readStructBegin();
      while (true)
      {
        schemeField = iprot.readFieldBegin();
        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) { 
          break;
        }
        switch (schemeField.id) {
          case 1: // HDFS_FILE_SPLIT
            if (schemeField.type == org.apache.thrift.protocol.TType.STRUCT) {
              struct.hdfs_file_split = new THdfsFileSplit();
              struct.hdfs_file_split.read(iprot);
              struct.setHdfs_file_splitIsSet(true);
            } else { 
              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
            }
            break;
          case 2: // HBASE_KEY_RANGE
            if (schemeField.type == org.apache.thrift.protocol.TType.STRUCT) {
              struct.hbase_key_range = new THBaseKeyRange();
              struct.hbase_key_range.read(iprot);
              struct.setHbase_key_rangeIsSet(true);
            } else { 
              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
            }
            break;
          default:
            org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);
        }
        iprot.readFieldEnd();
      }
      iprot.readStructEnd();

      // check for required fields of primitive type, which can't be checked in the validate method
      struct.validate();
    }

    public void write(org.apache.thrift.protocol.TProtocol oprot, TScanRange struct) throws org.apache.thrift.TException {
      struct.validate();

      oprot.writeStructBegin(STRUCT_DESC);
      if (struct.hdfs_file_split != null) {
        if (struct.isSetHdfs_file_split()) {
          oprot.writeFieldBegin(HDFS_FILE_SPLIT_FIELD_DESC);
          struct.hdfs_file_split.write(oprot);
          oprot.writeFieldEnd();
        }
      }
      if (struct.hbase_key_range != null) {
        if (struct.isSetHbase_key_range()) {
          oprot.writeFieldBegin(HBASE_KEY_RANGE_FIELD_DESC);
          struct.hbase_key_range.write(oprot);
          oprot.writeFieldEnd();
        }
      }
      oprot.writeFieldStop();
      oprot.writeStructEnd();
    }

  }

  private static class TScanRangeTupleSchemeFactory implements SchemeFactory {
    public TScanRangeTupleScheme getScheme() {
      return new TScanRangeTupleScheme();
    }
  }

  private static class TScanRangeTupleScheme extends TupleScheme<TScanRange> {

    @Override
    public void write(org.apache.thrift.protocol.TProtocol prot, TScanRange struct) throws org.apache.thrift.TException {
      TTupleProtocol oprot = (TTupleProtocol) prot;
      BitSet optionals = new BitSet();
      if (struct.isSetHdfs_file_split()) {
        optionals.set(0);
      }
      if (struct.isSetHbase_key_range()) {
        optionals.set(1);
      }
      oprot.writeBitSet(optionals, 2);
      if (struct.isSetHdfs_file_split()) {
        struct.hdfs_file_split.write(oprot);
      }
      if (struct.isSetHbase_key_range()) {
        struct.hbase_key_range.write(oprot);
      }
    }

    @Override
    public void read(org.apache.thrift.protocol.TProtocol prot, TScanRange struct) throws org.apache.thrift.TException {
      TTupleProtocol iprot = (TTupleProtocol) prot;
      BitSet incoming = iprot.readBitSet(2);
      if (incoming.get(0)) {
        struct.hdfs_file_split = new THdfsFileSplit();
        struct.hdfs_file_split.read(iprot);
        struct.setHdfs_file_splitIsSet(true);
      }
      if (incoming.get(1)) {
        struct.hbase_key_range = new THBaseKeyRange();
        struct.hbase_key_range.read(iprot);
        struct.setHbase_key_rangeIsSet(true);
      }
    }
  }

}

